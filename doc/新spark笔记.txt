standalone spark自带的调度框架
--------------------------------------------------------------
Spark安装
	tar -zxvf spark-2.4.0-bin-hadoop2.7.tgz -C /usr/local/
	cd /usr/local/spark-2.4.0-bin-hadoop2.7/conf
	cp spark-env.sh.template spark-env.sh
	cp slaves.template slaves

	vi spark-env.sh 在最后添加export信息
	export JAVA_HOME=/usr/local/jdk1.8.0_192  //导出java环境变量
	export SPARK_MASTER_HOST=node1    //配置master地址
	export SPARK_MASTER_PORT=7077     //配置master端口

	vi slaves
	node1
	node2
	node3
	noed4
	node5
	配置从节点
	分发配置文件
	for i in {2..5}; do scp spark-env.sh node$i:$PWD; done
	for i in {2..5}; do scp slaves node$i:$PWD; done
--------------------------------------------------------------
启动Spark
	sbin/start-all.sh
	网页访问 http://node1:8080/
停止Spark
	sbin/stop-all.sh
	sbin目录不建议配置到环境变量中
--------------------------------------------------------------
Spark配置高可用
	Spark的Worker推荐的Cores应该为Worker所在机器cpu的线程数
	vi conf/spark.env
	将export SPARK_MASTER_HOST=node1
	export SPARK_MASTER_PORT=7077注掉
	添加
	export SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node3,node4,node5 -Dspark.deploy.zookeeper.dir=/spark2"
	export SPARK_WORKER_CORES=1     //配置worder的可用核数
	export SPARK_WORKER_MEMORY=1g   //配置worker的内存
---------------------------------------------------------------
提交Spark官方示例到集群
bin/spark-submit --master spark://node1:7077,node2:7077 --class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.11-2.4.0.jar 100
求Pi，100次迭代

分配启动多少个Executor,每个worker用多少内存
bin/spark-submit --master spark://node1:7077,node2:7077 --class org.apache.spark.examples.SparkPi --executor-memory 1024mb --total-executor-cores 3  examples/jars/spark-examples_2.11-2.4.0.jar 10000

SparkSubmit(Driver)提交任务
Executor 执行真正的计算任务的

----------------------------------------------------------------
Spark Shell
spark shell是一个交互式的命令行，里面可以写spark程序，方便学习和测试,它也是一个客户端，用于提交spark应用程序
启动Spark Shell
bin/spark-shell   //本地模式启动shell
bin/spark-shell --master spark://node1:7077,node2:7077  
//提交任务到集群中，开始时sparksubmit（客户端）要连接Master，并申请计算资源（内存和核数），Master进行资源调度（就是让哪些Worker启动Exeutor），在准备工作时，这些进程都已经创建好了
-----------------------------------------------------------------
Spark读取hdfs上数据
整合高可用的HDFS，要把hdfs的core-site.xml和hdfs-site.xml两个文件拷贝到spark的conf目录下
Spark Shell执行wordcount
sc.textFile("hdfs://hdp12/wc").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collect
-----------------------------------------------------------------
将Driver端的一个集合通过并行化变成RDD，两种方式
val arr = Array(1,2,3,4,5,6,7,8,9,10)
val rdd1 = sc.parallelize(arr)
val rdd1 = sc.makeRDD(List(1,3,9,1,5,7,6))
-----------------------------------------------------------------
RDD上的算法
union方法，求并集
val rdd6 = sc.parallelize(List(5,6,4,7))
val rdd7 = sc.parallelize(List(1,2,3,4))

val rdd8 = rdd6.union(rdd7) //求并集
得Array[Int] = Array(5, 6, 4, 7, 1, 2, 3, 4)

-------------------------

intersection 方法，求交集
rdd9 = rdd6.intersection(rdd7)
得Array[Int] = Array(4)

-------------------------
join（连接）方法,join只能按照key等值join
val rdd1 = sc.parallelize(List(("tom", 1),("jerry", 2),("kitty",3)))
val rdd2 = sc.parallelize(List(("jerry",9),("tom", 8),("shuke", 7),("tom", 2)))
rdd1.join(rdd2)
得Array((tom,(1,2)), (tom,(1,8)), (jerry,(2,9)))
-------
leftOuterJoin 左外连接，左表的数据一定显示，右表的数据有就显示，没有就不显示
rdd1.leftOuterJoin(rdd2)
得Array((kitty,(3,None)), (tom,(1,Some(8))), (tom,(1,Some(2))), (jerry,(2,Some(9))))
-------
rightOuterJoin 右外连接，右表的数据一定显示，左表的数据有就显示，没有就不显示
rdd1.rightOuterJoin(rdd2)
得Array((tom,(Some(1),2)), (tom,(Some(1),8)), (jerry,(Some(2),9)), (shuke,(None,7)))
-------
cogroup 斜分组
rdd1.cogroup(rdd2)
得Array((kitty,(CompactBuffer(3),CompactBuffer())), (tom,(CompactBuffer(1),CompactBuffer(8, 2))), (jerry,(CompactBuffer(2),CompactBuffer(9))), (shuke,(CompactBuffer(),CompactBuffer(7))))

--------------------------

rdd2.join(rdd1)
得Array((tom,(2,1)), (tom,(8,1)), (jerry,(9,2)))

----------------------------------------------------------------------------

wordcount的几种方式
r1.flatMap(_.split(" ")).map((_, 1)).groupByKey().mapValues(_.sum).collect
r1.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_).collect
推荐使用reduceByKey，因为reduceByKey有局部聚合，所以在数据量大时比groupByKey效率高

reduceByKey(_+_,3)
				指定reduce的个数

reduceByKey(_+_,0) 用map输出
----------------------------------------------------------------------------
笛卡尔积
rdd1.cartesian(rdd2)
得
Array(((tom,1),(jerry,9)), ((tom,1),(tom,8)), ((tom,1),(shuke,7)), ((tom,1),(tom,2)), ((jerry,2),(jerry,9)), ((jerry,2),(tom,8)), ((jerry,2),(shuke,7)), ((jerry,2),(tom,2)), ((kitty,3),(jerry,9)), ((kitty,3),(tom,8)), ((kitty,3),(shuke,7)), ((kitty,3),(tom,2)))
---------------------------------------------------------------------------------

管线命令
对操作系统做命令
val rdd = sc.parallelize(Array("/Users/wangyadi"))
val rdd2: RDD[String] = rdd.pipe("ls")

------------------------------------------------------------------------------

coalesce减少分区,repartition再分区，可增可减
val conf = new SparkConf().setAppName("coalesceTest").setMaster("local[*]")
    val sc = new SparkContext(conf)

    val rdd1: RDD[String] = sc.parallelize(Array("hello world jack","hi tom marry hank","mydog is doudou","hay hank tom world"),4)

    println(s"rdd1 = ${rdd1.partitions.length}")

    val rdd2: RDD[(String, Int)] = rdd1.flatMap(_.split(" ")).map((_,1))

    println(s"rdd2 = ${rdd2.partitions.length}")

    val rdd3 = rdd2.coalesce(2)

    println(s"rdd3 = ${rdd3.partitions.length}")

    val rdd4 = rdd3.repartition(5)

    println(s"rdd4 = ${rdd4.partitions.length}")

----------------------------------------------------------------------------------------
RDD的算子分为两类，一类是Transformation(lazy)，一类是Action（触发任务执行）
RDD不存在真正要计算的数据，而是记录了RDD的转换关系，调用了什么方法，传入什么函数
---------------------------------------------------------------------------------
SparkShell中通过parallelize将本地集合并行化时，默认的分区是sparkshell启动时配置的cores数
---------------------------------------------------------------------------------
rdd.partitions.length 获得RDD分区的数量
Spark允许最小的分区数量是2
---------------------------------------------------------------------------------
top(3)，将集合排序，返回前三个
take(2), 取集合里的前两个
takeOrdered(3)  将集合排序，返回前三个，默认升序
---------------------------------------------------------------------------------
RDD的map方法，真正在Executor中执行时，是一条一条的将数据拿出来处理

mapPartitionsWithIndex 一次拿出一个分区（分区中并没有数据，而是记录要读取哪些数据，真正生成的Task会读取多条数据），并且可以将分区的编号取出来

功能：取分区中对应的数据时，还可以将分区的编号取出来，这样就可以知道数据是属于哪个分区的（哪个分区对于的Task的数据）
val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)
val func = (index: Int, it: Iterator[Int]) => {
    it.map(e => s"part: $index, ele: $e")
}
val rdd2 = rdd.mapPartitionsWithIndex(func)
rdd2.collect
得到
Array(part: 0, ele: 1, part: 0, ele: 2, part: 0, ele: 3, part: 0, ele: 4, part: 1, ele: 5, part: 1, ele: 6, part: 1, ele: 7, part: 1, ele: 8, part: 1, ele: 9)
----------------------------------------------------------------------------------
rdd.aggregate(0)(_+_,_+_)
            初始值 局部 全局
rdd.aggregate(0)(math.max(_,_),_+_)
局部和全局函数都会应用初始值
--------------------------------------------------------------------------------
aggregateByKey 
val pairRDD = sc.parallelize(List(("cat",2),("cat",5),("mouse",4),("dog",12),("mouse",2)),2)
现在要按key相加
pairRDD.aggregateByKey(0)(_+_,_+_).collect
                     注意：初始值只会在局部聚合中使用，不会在全局聚合中使用
--------------------------------------------------------------------------------
collectAsMap 阻塞方法？

--------------------------------------------------------------------------------
val rdd3 = sc.parallelize(List(("a",1),("b",2),("b",2),("c",2),("c",1)))
rdd3.countByKey()
得到 scala.collection.Map[String,Long] = Map(a -> 1, b -> 2, c -> 2)
注意统计的是key出现的次数，和value没关系

rdd3.filterByRange("a","b")
得到Array[(String, Int)] = Array((a,1), (b,2), (b,2))

------------------------------------------------------------------------------
flatMapValues(_.split(" "))
对元组里的values进行flatMap

----------------------------------------------------------------------------
foldByKey
val rdd4 = sc.parallelize(List("dog","wolf","cat","bear"),2)
val rdd5 = rdd4.map(x => (x.length,x))
rdd5.foldByKey("")(_+_)
得到Array[(Int, String)] = Array((4,wolfbear), (3,catdog))

aggregateByKey("")(seqOp,comOp)  reduceByKey()   foldByKey   combineByKey调用的都是combineByKeyWithClassTag

-------------------------------------------------------------------------------
需要重点理解的方法
aggregate 只能对RDD[U] 里面只有一个元素的RDD进行操作  Action方法
aggregateByKey 只能对RDD[(K,U)]进行操作   Transformation方法
reduceByKey    Transformation方法

map            Transformation方法
mapValues      Transformation方法
mapPartions    Transformation方法
mapPartionsWithIndex    Transformation方法

filter  Transformation方法

foreach        Action方法
foreachPartition     Action方法
saveAsTextFile   Action方法
collect         Action方法
---------------------------------------------------------------------------------
val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)
rdd.foreach(e => println(e*100+""))
数据打印在Executor端

------------------------------------------------------------------------------
rdd.foreachPartition(it => it.foreach(x => println(x * 100))) //异步方法  Action
每次拿出一个分区进行操作

foreach一条一条拿，foreachPartition一次拿一个分区
-----------------------------------------------------------------------------------
拉链操作 zip
val rdd4 = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"),3)
val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2),3)
val rdd6 = rdd5.zip(rdd4)
得到Array((1,dog), (1,cat), (2,gnu), (2,salmon), (2,rabbit), (1,turkey), (2,wolf), (2,bear), (2,bee))
------------------------------------------------------------------------
combinByKey详情见combinByKey详解.png
------------------------------------------------------------------------------
Spark自定义分区器
定义一个类，继承org.apache.spark.Partitioner
实现
	def numPartitions: Int
	def getPartition(key: Any): Int方法
在rdd中使用 rdd.partitionBy(new MyPartitioner)返回一个新的已经分好区的RDD，上游在shuffle的时候调用自定义的分区器分区
上游分区好以后，把信息告诉Driver，下游读取Driver的信息后到相应位置拉取数据
---------------------------------------------------------------------------------
Spark Cache
var lines = sc.textFile()
val cached = lines.cache()  //标记这个RDD以后执行时会被cache，lazy
lines.count  //触发Action存内存
在管理界面Storage标签里看被缓存的数据
数据缓存在Executor端

cached.unpersist(true) //同步释放缓存
cached.unpersist(false) //异步执行释放缓存

什么时候进行cache
	1.要求计算速度快
	2.集群资源要足够大
	3.重要：cache的数据会多次触发Action
	4.先进行过滤，然后将缩小范围的数据再cache到内存
----------------------------------------------------------------------------------
分布式内存文件系统
Tachyon 改名
alluxio
-----------------------------------------------------------------------------------
checkpoint
什么时候做checkpoint
	1.迭代计算，要求保证数据安全
	2.对速度要求不高（跟cache到内存进行对比）
	3.将中间结果保存到hdfs
sc.setCheckpointDir("hdfs://hdp12/ck20190131")
rdd.checkpoint  //lazy方法

//设置checkpoint目录（分布式文件系统的目录hdfs目录）
//经过复杂计算，得到中间结果
//将中间结果checkpoint到指定的hdfs目录
//后续的计算，就可以使用前面ck的数据了
//一个rdd同时有被persist和checkpoint，优先使用cache
------------------------------------------------------------------------------------
广播变量
val broadcastRules: Broadcast[Array[(Long, Long, String)]] = sc.broadcast(rules)
调用broadcastRules.value就可以在Executor中拿到

//Task是在Driver端生成的，广播变量的引用是伴随着Task被发送到Executor中的
-----------------------------------------------------------------------------
case class function1(name: String) 构造器参数不用加val/var修饰
----------------------------------------------------------------------------------
Spark从Mysql中读取数据
创建org.apache.spark.rdd.JdbcRDD
new JdbcRDD(sc,() => {DriverManager.getConnection("dbc:mysql://node3:3306/bigdata?useUnicode=true&characterEncoding=utf8","root","password")},"SELECT * FROM TABLE WHERE id >= ? AND id <= ?",1L,100L,2,rs => {
	val id = rs.getLong(1)
	val name = rs.getString(2)
	val age = rs.getInt(3)
	(id,name,age)
})
jdbcRDD的小问题
where 一定要用 >=  <= 包含，否则，rdd分区在读数据的时候会运用不包含丢数据
------------------------------------------------------------------------------------
Spark任务执行的流程

四个步骤
1.构建DAG（调用RDD上的方法）
2.DAGScheduler将DAG切分Stage（切分的依据是Shuffle），生成Task以TaskSet的形式给TaskScheduler
3.TaskScheduler调度Task（根据资源情况将Task调度到相应的Executor中执行）
4.Executor接收Task，然后将Task丢入到线程池中执行

----------------------------------------------------------------------------------------
1.构建DAG
DAG 有向无环图（数据执行过程，有方向，无闭环）

DAG描述多个RDD的转换过程，任务执行时，可以按照DAG的描述，执行真正的计算（数据被操作的过程）

DAG是有边界的：开始（通过SparkContext创建的RDD），结束（触发Action，调用runJob），一个完整的DAG就形成了

一个RDD只是描述了数据计算过程中的一个环节，而DAG由一到多个RDD组成，描述了数据计算过程中的所有环节（过程）

一个Spark任务Application中有多少个DAG，有一到多个（取决于触发了多少次Action）

-----------------------------------------------------------------------------------------
2.DAGScheduler
一个DAG中可能产生多种不同类型和功能的Task，会有不同的阶段

DAGScheduler：将一个DAG切分成一到多个Stage，DAGScheduler切分的依据是Shuffle（宽依赖）

为什么要切分Stage？
	一个复杂的业务逻辑（将多台机器上具有相同数据的数据聚合到一台机器上：Shuffle）
	如果有shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，下一个阶段的计算要依赖上一个阶段的数据
	在同一个Stage中，会有多个算子，我们称为pipeline（流水线：严格按照流程、顺序执行），多个算子可以被合并在一起
---------------------------
Narrow Dependencies(窄依赖)
map,filter union
特殊情况 join with inputs co-partitioned
--------------------------
Shuffle的定义
	shuffle的含义是洗牌，将数据打散，父RDD一个分区中的数据如果给了子RDD的多个分区，只要存在这种可能，就是shuffle
	shuffle会有网络传输，但是有网络传输并不意味着就是shuffle
-------------------------
Wide Dependencies(宽依赖)
groupByKey join with no-co-partitioned

--------------------------------------------------------------------------
spark Executor中引用Driver端变量的问题
见 每读取一条数据new一个rules.png 每个Task有独立的rules.png  一个Executor中多个Task共用一个rules.png  rules在Executor中实例化.png

---------------------------------------------------------------------------------
SimpleDateFormat是线程不安全的，如果多个Task在一个Executor中同时使用，要用线程安全的
用FastDateFormat就是线程安全的

---------------------------------------------------------------------------------
SparkSQL是spark上的高级模块，SparkSQL是一个SQL解析引擎，将SQL解析成特殊的RDD(DataFrame)，然后在Spark集群中运行

SparkSQL是用来处理结构化数据的，（先将非结构化的数据转换成结构化数据）

SparkSQL支持两种编程API
	1.SQL方式
	2.DataFrame的方式（DSL）

SparkSQL兼容hive，元数据库、SQL语法、UDF】序列化，反序列化

SparkSQL支持统一的数据源，可以读取多种类型的数据

SparkSQL提供了标准的连接（JDBC、ODBC），以后可以对接BI工具

----------------------------------------------------------------------------------------

RDD和DataFrame的区别

DataFrame里存放的是结构化数据的描述信息，DataFrame要有表头（表的描述信息），描述了有多少列，每一列叫什么名字，什么类型，能不能为空

DataFrame是一个特殊的RDD（RDD+Schema信息就变成了DataFrame）

----------------------------------------------------------------------------------------
SparkSql的第一个入门程序

首先在pom中添加sparkSQL的依赖
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-sql_2.11</artifactId>
    <version>${spark.version}</version>
</dependency>

SparkSQL 1.x和2.x的变成API有一些变化，企业中都有使用

先用1.x的方式
	创建一个SQLContext

	//创建SparkSql的连接
    //sparkcontext不能创建特殊的rdd（DataFrame）
    //将sparkContext包装进而增强
	val sc = new SparkContext(conf)
	val sqlContext = new SQLContext(sc)

	//创建特殊的RDD，先有一个普通的RDD，然后再关联上schema信息，进而转换成DataFrame
	val lines: RDD[String] = sc.textFile(args(0))
	//把数据关联case class或普通的class 将非结构化数据变成结构化数据

	case class Boy(id: Long, name: String, age: Int, fv: Double)

	val boys: RDD[Boy] = lines.map(line => {
      val fields = line.split(",")
      val id = fields(0).toLong
      val name = fields(1)
      val age = fields(2).toInt
      val fv = fields(3).toDouble

      Boy(id, name, age, fv)
    })

    //该RDD装的是Boy类型的数据，有了schema信息，但是还是一个RDD
    //将RDD转换成DataFrame
    import sqlContext.implicits._
    val boyFrame: DataFrame = boys.toDF

    //变成DF后就可以使用两种API进行编程了
    //第一种方式，sql的方式
    //先注册临时表，把DataFrame注册成临时表
    boyFrame.registerTempTable("t_boy")
    //书写SQL（SQL方法是Transformation）
    val reulst: DataFrame = sqlContext.sql("SELECT * FROM t_boy ORDER BY fv DESC,age ASC")

    reulst.show()
------------------------------------------------------------------------------------------------

将普通rdd变成dataframe的第二种方式
val boys: RDD[Row] = lines.map(line => {
      val fields = line.split(",")
      val id = fields(0).toLong
      val name = fields(1)
      val age = fields(2).toInt
      val fv = fields(3).toDouble

      Row(id, name, age, fv)
    })
    //该RDD装的是Boy类型的数据，有了schema信息，但是还是一个RDD
    //将RDD转换成DataFrame第二种方式

    val schema: StructType = StructType(List(
      new StructField("id", LongType, false),
      new StructField("name", StringType, true),
      new StructField("age", IntegerType, true),
      new StructField("fv", DoubleType, true)
    ))

    val boyFrame: DataFrame = sqlContext.createDataFrame(boys,schema)

------------------------------------------------------------------------------------------------
DSL(DataFrame API)
导入隐式转换
import  sqlContext.implicits._
val df1: DataFrame = boyFrame.select("name","age","fv")
//val whered = df1.where($"fv" > 99 )
val ordered: Dataset[Row] = df1.orderBy($"fv" desc,$"age" asc)


2.x的方式
	创建一个SparkSession
val spark: SparkSession = SparkSession.builder().appName("sqlWordCount").master("local[*]").getOrCreate()
//Dataset也是分布式数据集，是对RDD进一步封装，是更加只能的RDD
val lines: Dataset[String] = spark.read.textFile(args(0))

----------------------------------------------------------------------------------
join的代价太昂贵，而且非常慢，解决思路是将小表缓存起来（广播变量）

Spark自定义函数 （UDF，UDAF）
	UDF   输入一行，返回一个结果  一对一
	UDTF  输入一行，返回多行     一对多
	UDAF  输入多行，返回一行     多对一

UDF
定义并注册
	spark.udf.register("ip2Province",(ipNumber: Long) => {
      //查找ip规则，而ip规则事先已经广播了，已经在Executor中了
      //函数的逻辑是在Executor中执行的
      val index: Int = IpUtil.binarySearch(broadcastRef.value,ipNumber)
      var province = "未知"
      if(index != -1){
        province = (broadcastRef.value)(index)._3
      }
      province
    })
调用
	val result: DataFrame = ipDF.select(callUDF("ip2Province",$"ip") as "cityName")
      .groupBy($"cityName").agg(count("*") as "cnts").orderBy($"cnts" desc)
    result.show()
---------------------------------------

UDTF 在hive中有，SparkSQL中没有
Spark中用flatMap即可实现该功能

---------------------------------------

UDAF 自定义聚合函数  
count sum avg max min这些是sparkSQL自带的聚合函数，但是复杂的业务，要自己定义

自定义一个类，继承UserDefinedAggregateFunction
实现8个方法
class GeoMean extends UserDefinedAggregateFunction {

  //输入数据的类型
  override def inputSchema: StructType = StructType(List(StructField("value",DoubleType)))
  //产生中间结果的数据类型
  override def bufferSchema: StructType = StructType(List(
    //参与运算数字的个数
    StructField("counts", LongType),
    //相乘之后返回的积
    StructField("product", DoubleType)
  ))

  //最终返回的结果类型
  override def dataType: DataType = DoubleType


  override def deterministic: Boolean = true

  //指定初始值
  override def initialize(buffer: MutableAggregationBuffer): Unit = {
    //参与运算数字个数的初始值
    buffer(0) = 0L
    //参与相乘的初始值
    buffer(1) = 1.0
  }

  //每有一条数据参与运算就更新一下中间结果（update相当于在每一个分区中的运算）
  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    //参与运算的个数更新
    buffer(0) = buffer.getLong(0) + 1L
    //每有一个数字参与运算就进行相乘（包含中间结果）
    buffer(1) = buffer.getDouble(1) * input.getDouble(0)
  }

  //全局聚合
  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    //每个分区参与运算的中间结果进行相加
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    //每个分区计算的结果进行相乘
    buffer1(1) = buffer1.getDouble(1) * buffer2.getDouble(1)
  }

  //计算最终结果
  override def evaluate(buffer: Row): Any = {
    Math.pow(buffer.getDouble(1),1.toDouble/buffer.getLong(0))
  }
}

调用有两种方式
方式一、
val geomean = new GeoMean
spark.udf.register("gm",geomean)

import org.apache.spark.sql.functions._
import spark.implicits._

val powData: DataFrame = range.select(callUDF("gm",$"id") as "result")

方式二、
val geomean = new GeoMean
import spark.implicits._
val powData = range.agg(geomean($"id").as("result"))

------------------------------------------------------------------------------------

Dataset是spark1.6以后推出的新的API，也是一个分布式数据集，于RDD相比，保存了很多的描述信息，概念上等同于关系型数据库中的二维表，基于保存了很多的描述信息，spark在运行时可以被优化。

Dataset里面对应的数据时强类型的，并且可以使用功能更加丰富的lambda表达式，弥补了函数式编程的一些缺点，使用起来更加方便

在scala中，DataFrame其实就是Dataset[Row]

Dataset的特点：
	1.一系列分区
	2.每个切片上会有对应的函数
	3.依赖关系
	4.kv类型shuffle也会有分区器
	5.如果读取hdfs中的数据会感知最优位置
	6.会优化执行计划
	7.支持智能数据源

调用Dataset的方法先会生成逻辑计划，然后被spark的优化器进行优化，最终生成物理计划，然后提交集群中运行

硬编码要执行的更快，spark dataset把手写的代码转换成硬编码执行  

--------------------------------------------------------------------------------------------

sparkSession读取jdbc数据源
val access: DataFrame = spark.read.format("jdbc").options(
      Map("url" -> "jdbc:mysql://192.168.56.103:3306/bigdata",
        "driver" -> "com.mysql.jdbc.Driver",
        "dbtable" -> "access",
        "user" -> "root",
        "password" -> "az63091919")
    ).load()

-----------------------------------------------------------------------------------------------

保存到jdbc
//保存到jdbc
    val props = new Properties()
    props.put("user","root")
    props.put("password","az63091919")
    fitler.write.mode("ignore").jdbc("jdbc:mysql://192.168.56.103:3306/bigdata?useUnicode=true&characterEncoding=utf8","access1",props)

mode：
	overwrite  覆盖
	append     追加
	ignore     如果表存在，不做任何操作，如果表不存在自动创建表并写入数据
	error

------------------------------------------------------------------------------------------------

保存到文件里，Dataset或DataFrame只能有一列，且是String类型的
fitler.write.text("/Users/wangyadi/Desktop/text")

-------------------------------------------------------------------------------------------

保存到json里
    access.write.json("/Users/wangyadi/Desktop/json")

-----------------------------------------------------------------------------------------------------

保存到csv里
    access.write.csv("/Users/wangyadi/Desktop/csv")

-----------------------------------------------------------------------------------------------------

保存到parqut里，智能数据源
    access.write.parquet("/Users/wangyadi/Desktop/parquet")

-------------------------------------------------------------------------------------------------------

读取json数据源
	val jsonData = spark.read.json("/Users/wangyadi/Desktop/json")

-----------------------------------------------------------------------------------------------------

读取csv数据源
	val csv: DataFrame = spark.read.csv("/Users/wangyadi/Desktop/csv")

    csv.printSchema()

    val result: DataFrame = csv.toDF("cityName","cnts")

    result.show()

csv读出来的数据都没有schema信息，且都是String类型

---------------------------------------------------------------------------------------------------------------

读取parqut文件
	val parquetLine: DataFrame = spark.read.parquet("/Users/wangyadi/Desktop/parquet")

    parquetLine.show()

-----------------------------------------------------------------------------------------------------------

Spark SQL DSL风格窗口函数
.select($"subject",$"teacher",$"cnts"                                                                   
  ,row_number().over(Window.partitionBy($"subject").orderBy($"cnts" desc)).as("sub_rk")                 
  ,row_number().over(Window.orderBy($"cnts" desc)) as "g_rk")                                           

-------------------------------------------------------------------------------------------------------------

spark三种join实现

Broadcast Join
	将小表广播到Executor中，大表在每个分区中和小表join
//强制设置BroadcastJoin执行，最大缓存10M
    spark.conf.set("spark.sql.autoBroadcastJoinThreshold",1024*1024*10)
----------------------

Shuffle Hash Join
	两个表按照相同的Partitioner分区，再join

	spark2.0默认 BroadcaHashJoin

-----------------------

Sort Merge Join
	两张大表，先全局排序，再join，rdd默认排序用rangePartitioner

	//强制设置SortMergeJoin执行
    spark.conf.set("spark.sql.autoBroadcastJoinThreshold",-1)

-----------------------
在spark2.0中，broadcast Join和Shuffle Hash Join合并为 BroadcastHashJoin

-------------------------------------------------------------------------------------------------------------

Hive On Spark

把hive安装包下的hive-sit.xml拷贝到spark的conf目录下

./spark-sql --master spark://node-4:7077,node-5:7077 --driver-class-path /home/xiaoniu/mysql-connector-java-5.1.7-bin.jar

sparkSQL会在mysql上创建一个database，需要手动改一下DBS表中的DB_LOCATION_UIR改成hdfs的地址

添加export HADOOP_CONF_DIR=/usr/local/hadoop2.8.5/etc/hadoop
-------------------------------------------------------------------------------------------------------------

Driver端连接mysql数据库，获取schema信息

----------------------------------------------------------------------------------------------------------

spark-sql  
	-e  SQL from command line
	-f filename  SQL from files

//在代码中启用spark对hive的支持

在pom中加入
<dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.11</artifactId>
            <version>${spark.version}</version>
        </dependency>

//想要使用hive的元数据库，必须指定hive元数据库的位置，添加一个hive-site.xml到当前程序的classpath下即可
//想要访问hdfs中存储的数据，必须把hdfs-site.xml和core-site.xml

val spark = SparkSession
    .builder().appName("HiveOnSpark").master("local[*]")
    .enableHiveSupport().getOrCreate()

创建表的时候需要hadoop用户身份
System.setProperty("HADOOP_USER_NAME", "root") // 伪装客户端的用户身份为root
//  或者添加运行参数 –DHADOOP_USER_NAME=root
------------------------------------------------------------------------------------------------------------

Kafka的一些概念
	Broker：按装Kafka服务的那台机器就是一个broker（broker的id要全局唯一）
	Producer：消息的生产者，负责将数据写入到broker中（push）
	Consumer：消息的消费者，负责从kafka中读取数据（pull），老版本的消费者需要依赖zk，新版本的不需要
	Topic：主题，相当于是数据的一个分类，不同的topic存放不同的数据
	Consumer Group：消费者组，一个Topic可以有多个消费者同时消费，多个消费者如果在一个消费者组中，那么它们不能重复消费数据

------------------------------------------------------------------------------------------------------------

Kafka0.8安装
修改config/server.properties
	broker.id=0  要全局唯一
	log.dirs=    kafka存数据的地址
	num.partitions=   kafka的物理分区
	log.retention.hours=168  kafka数据保存的策略默认168小时
	zookeeper.connect=node3:2181,node4:2181,node5:2181  zookeeper的地址

------------------------------------------------------------------------------------------------------------

启动Kafka0.8
	进入到kafka0.8安装目录，输入
	bin/kafka-server-start.sh -daemon config/server.properties  后台启动kafka

Kafka命令行操作

列出当前有哪些topic
	bin/kafka-topics.sh --list --zookeeper node3:2181,node4:2181,node5:2181
-----------------------
创建topic
	bin/kafka-topics.sh --create --zookeeper node3:2181,node4:2181,node5:2181 --replication-factor 3 --partitions 3 --topic mytopic

	--replication-factor  //存几分副本
	--partitions  //几个分区
------------------------
查看topic详细信息
bin/kafka-topics.sh --describe --zookeeper node3:2181,node4:2181,node5:2181 --topic mytopic
------------------------
向topic里写入数据
bin/kafka-console-producer.sh --broker-list node2:9092,node3:9092,node4:9092 --topic mytopic
--------------------------
从头消费topic的数据
bin/kafka-console-consumer.sh --zookeeper node3:2181,node4:2181,node5:2181 --topic mytopic --from beginning
从启动开始消费topic的数据
bin/kafka-console-consumer.sh --zookeeper node3:2181,node4:2181,node5:2181 --topic mytopic

------------------------------------------------------------------------------------------------------------

Kafka进程是不分主从，但是分区有leader分区和follower分区，leader分区负责读写，follower分区负责同步数据。
Kafka可以将读写的压力均摊到多台机器上，从而提高了读写能力。
在生产环境上配置副本保存3份，分区的数量是broker的数量*每台机器上可用的核数
从分区负责同步数据，也可以配置从分区负责读

---------------------

Topic:mytopic	PartitionCount:3	ReplicationFactor:3	Configs:
	Topic: mytopic	Partition: 0	Leader: 0	Replicas: 0,1,2	Isr: 0,1,2
	Topic: mytopic	Partition: 1	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
	Topic: mytopic	Partition: 2	Leader: 2	Replicas: 2,0,1	Isr: 2,0,1

mytopic有三个分区
	0号分区 Leader在borker.id=0的机器上，副本在0，1，2  副本传递顺序是 0，1，2

----------------------------------------------------------------------------------------------------------

spark steaming工作最少需要两个task，一个是receiver（负责接收数据）一个是calculater（负责业务逻辑），详见nc命令图解

----------------------------------------------------------------------------------------------------------

spark steaming与kafka0.8版本集成，receiver方式

val conf = new SparkConf().setAppName("KafkaWordCount").setMaster("local[*]")

    val ssc = new StreamingContext(conf, Seconds(5))

    val zkQuorm = "node3:2181,node4:2181,node5:2181"
    val groupId = "g1"
    //topic和对应的线程
    val topic = Map[String,Int]("mytopic" -> 1)

    //创建DStream，需要KafkaDStream
    //Kafka的ReceiverInputDStream里面装的是元组，key是往kafka里写的时候的key，value是实际写入的值
    val kafkaData: ReceiverInputDStream[(String, String)] = KafkaUtils.createStream(ssc, zkQuorm, groupId, topic)

----------------------------------------------------------------------------------------------------------

做累加wordcount
用updateStateByKey，但是这种方式已经过时了
    val reduced: DStream[(String, Int)] = wordAndOne.updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true)

/**
    * 第一个参数：聚合的key，就是单词
    * 第二个参数：当前批次该单词在每一个分区出现的次数
    * 第三个参数：初始值或累加的中间结果
    */
  val updateFunc = (iter: Iterator[(String,Seq[Int],Option[(Int)])]) => {
    //iter.map(t => (t._1,t._2.sum + t._3.getOrElse(0))) 或使用模式匹配
    iter.map{
      case(x,y,z) => (x, y.sum + z.getOrElse(0))
    }
  }

------------------------------------------------------------------------------------------------------------

深入理解DStream，他是sparkStreaming中的一个最基本的抽象，代表了一系列连续的数据，本质上是一系列连续的RDD，你对DStream进行操作，就是对RDD进行操作

每隔一个固定的时间，就会生成一个小的DAG（是因为RDD里面要计算对应的数据少），然后将这个小DAG提交到集群中，周而复始
详见DStream图解.png

DStream每隔一段时间生成一个RDD，你对DStream进行操作，本质上是对里面的对应时间的RDD进行操作

DStream可以认为是一个RDD的工厂，该DStream里面生产的都是相同业务逻辑的RDD，只不过是RDD里面要读取的数据不相同

DStream和DStream之间存在依赖关系，在一个固定的时间点，对应存在依赖关系的DStream对应的RDD也存在依赖关系
每隔一个固定的时间，其实产生了一个小的DAG，周期性的将生成的小DAG提交到集群中执行

-----------------------------------------------------------------------------------------------------------

SparkStreaming的Receiver方式和直连方式有什么区别
	Receiver接收固定时间间隔的数据（放在内存中的），使用Kafka高级的API，自动维护偏移量达到固定的时间才进行处理，效率低并且容易丢失数据
	Direct直连方式，相当于RDD直接连接到Kafka的分区上，使用Kafka底层的API，效率高，需要自己维护偏移量。

-----------------------------------------------------------------------------------------------------------

spark 整合kafka0.8
用zookeeper管理偏移量
见SparkTest com.wyd.day09.KafkaDirectWordCountV2
def main(args: Array[String]): Unit = {
    //指定组名
    val group = "a001"
    //创建SparkConf
    val conf = new SparkConf().setAppName("KafkaDirectWordCount").setMaster("local[*]")
    //创建SparkStreaming，并设置间隔时间
    val ssc = new StreamingContext(conf, Duration(5000))
    //指定消费的topic名字
    val topic = "wordcount"
    //指定Kafka的broker地址
    val brokerList = "node2:9092,node3:9092,node4:9092"
    //指定zk的地址，后期更新消费的偏移量时使用（以后可以使用Redis、MySQL来记录偏移量）
    val zkQuorum = "node3:2181,node4:2181,node5:2181"
    //创建stream时使用的topic名字集合，SparkStreaming可以同时消费多个topic
    val topics: Set[String] = Set(topic)

    //创建一个ZKGroupTopicDirs对象
    val topicDirs = new ZKGroupTopicDirs(group, topic)
    //获取zookeeper中的路径 "/a001/offsets/wordcount"
    val zkTopicPath = s"${topicDirs.consumerOffsetDir}"

    //准备kafka的参数
    //smallest代表最开始
    val kafkaParams = Map(
      "metadata.broker.list" -> brokerList,
      "group.id" -> group,
      "auto.offset.reset" -> kafka.api.OffsetRequest.SmallestTimeString
    )

    //zookeeper客户端，可以从zk中读取偏移量，并更新偏移量
    val zKClient = new ZkClient(zkQuorum)
    //查询该路径下是否字节点（默认有字节点为我们自己保存不同partition时生成）
    //  组  /       / topic   /分区/ 偏移量
    // /a001/offsets/wordcount/0/10001
    // /a001/offsets/wordcount/1/61201
    // /a001/offsets/wordcount/2/30001
    val children: Int = zKClient.countChildren(zkTopicPath)

    var kafkaStream: InputDStream[(String,String)] = null
    //如果zookeeper中保存offset，我们会利用这个offset作为kafkaStream的起始位置
    var fromOffsets: Map[TopicAndPartition, Long] = Map()

    if(children > 0) {
      for(i <- 0 until children) {
        val partitionOffset: String = zKClient.readData[String](s"$zkTopicPath/${i}")
        val tp: TopicAndPartition = TopicAndPartition(topic, i)
        fromOffsets += (tp -> partitionOffset.toLong)
      }
      //Key:kafka的key  values：内容"hello tom hell jerry"
      //这个会将kafka的消息进行transform，最终kafka的数据会变成（kafka的key，message）这样的tuple
      val messageHandler = (mmd: MessageAndMetadata[String, String]) => (mmd.key(), mmd.message())

      kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder, (String, String)](ssc, kafkaParams, fromOffsets, messageHandler)

    } else {
      //从头读，                    kafka中的key，v   key的解析器      v的解析器
      kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc,kafkaParams,topics)
    }

    var offsetRanges = Array[OffsetRange]()

    //直连方式只有在KafkaDStream的RDD中才能获取偏移量，那么就不能调用DStream的Transformation
    //所以只能在KafkaStream中调用foreachRDD，获取RDD的偏移量，然后就是对RDD进行操作了
    //依次迭代KafkaDStream中的KafkaRDD
    //KafkaStream.foreachRDD里面的业务逻辑是在Driver端执行的
    kafkaStream.foreachRDD(kafkaRDD => {
      //只有kafkaRDD实现了HasOffsetRanges特质
      offsetRanges = kafkaRDD.asInstanceOf[HasOffsetRanges].offsetRanges
      //.map()是在executer中执行
      val message: RDD[String] = kafkaRDD.map(_._2)

      message.foreachPartition(partition => {
        partition.foreach(x => {
          println(x)
        })
      })

      for (o <- offsetRanges) {
        val zkPath = s"${topicDirs.consumerOffsetDir}/${o.partition}"
        ZkUtils.updatePersistentPath(zKClient, zkPath, o.untilOffset.toString)
      }
    })
    ssc.start()
    ssc.awaitTermination()
  }

还可以用checkpoint存储偏移量，后续调查？这种方式已经过时

------------------------------------

spark整合kafka0.10，
1、利用kafka自己存储偏移量
见sparkkafka010  com.wyd.spark.MyWordCount

CREATE TABLE `offset` (
  `groupId` varchar(255) DEFAULT NULL,
  `topic` varchar(255) DEFAULT NULL,
  `partition` int(11) DEFAULT NULL,
  `untilOffset` bigint(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8

object MyWordCount {
  var stopFlag:Boolean = false
  val shutdownMarker = "hdfs://hdp12/tmp/shutdownmarker"

  def main(args: Array[String]): Unit = {

    val group = "g0"
    val topic = "mywordcount"

    val conf = new SparkConf().setAppName("MyWordCount")

    val streamingContext = new StreamingContext(conf, Seconds(5))

    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "node2:9092,node3:9092,node4:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> group,
      "auto.offset.reset" -> "earliest",  //lastest
      "enable.auto.commit" -> (false: java.lang.Boolean)
    )

    val topics = Array(topic)

    val stream = KafkaUtils.createDirectStream[String, String](
      streamingContext,
      //位置策略（如果kafka和spark程序部署在一起，会有最优位置感知）
      LocationStrategies.PreferConsistent,
      //订阅的策略（可以指定用正则的方式读取topic，比如my-orders-*）
      ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
    )

    stream.foreachRDD(kafkaRDD => {
      if(!kafkaRDD.isEmpty()) {
        val offsetRanges = kafkaRDD.asInstanceOf[HasOffsetRanges].offsetRanges
        val lines: RDD[String] = kafkaRDD.map(x => x.value())
        val words: RDD[String] = lines.flatMap(_.split(" "))
        val wordAndOne = words.map((_,1))
        val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_+_)
        reduced.foreachPartition(it => {
          val conn: Jedis = JedisConnectionPool.getConnection()
          it.foreach(x => {
            conn.incrBy(x._1,x._2.toLong)
          })
          conn.close()
        })
        //更新偏移量
        stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
      }
    })

    streamingContext.start()

    val checkIntervalMillis = 10000
    var isStopped = false


    while (! isStopped) {
      println("calling awaitTerminationOrTimeout")
      isStopped = streamingContext.awaitTerminationOrTimeout(checkIntervalMillis)
      if (isStopped)
        println("confirmed! The streaming context is stopped. Exiting application...")
      else
        println("Streaming App is still running. Timeout...")
      checkShutdownMarker
      if (!isStopped && stopFlag) {
        println("stopping ssc right now")
        streamingContext.stop(true, true)
        println("ssc is stopped!!!!!!!")
      }
    }
  }

  def checkShutdownMarker = {
    if (!stopFlag) {
      val fs = FileSystem.get(new Configuration())
      stopFlag = fs.exists(new Path(shutdownMarker))
    }

  }
}

2、用关系型数据库存储偏移量
见sparkkafka com.wyd.spark.OrderCalculate

object OrderCalculate {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("OrderCalculate").setMaster("local[*]")

    val streamingContext = new StreamingContext(conf,Duration(5000))

    //获取ip规则，然后广播
    val lines = streamingContext.sparkContext.textFile("/Users/wangyadi/sparkData/ip/rule")
    val rulesRdd: RDD[(Long, Long, String)] = lines.map(line => {
      val fileds: Array[String] = line.split("[|]")
      (fileds(2).toLong, fileds(3).toLong, fileds(6))
    })
    val rules: Array[(Long, Long, String)] = rulesRdd.collect()
    //调用sc上的广播方法，将Driver端的数据广播到executor中
    val broadcastRules: Broadcast[Array[(Long, Long, String)]] = streamingContext.sparkContext.broadcast(rules)

    val groupId = "ag1"

    val kafkaParams = Map[String, Object](
      "bootstrap.servers" -> "node2:9092,node3:9092,node4:9092",
      "key.deserializer" -> classOf[StringDeserializer],
      "value.deserializer" -> classOf[StringDeserializer],
      "group.id" -> groupId,
      "auto.offset.reset" -> "earliest",
      "enable.auto.commit" -> (false: java.lang.Boolean)
    )

    val topics = Array("orderList")

    DBs.setup()
    val fromdbOffset: Map[TopicPartition, Long] = DB.readOnly(
      implicit session => {
        SQL(s"select * from offset where groupId = '${groupId}'")
          .map(rs => (new TopicPartition(rs.string("topic"), rs.int("partition")), rs.long("untilOffset")))
          .list().apply()
      }
    ).toMap

    val stream = if(fromdbOffset.size == 0){
      KafkaUtils.createDirectStream[String, String](
        streamingContext,
        LocationStrategies.PreferConsistent,
        ConsumerStrategies.Subscribe[String, String](topics, kafkaParams)
      )
    } else {
      KafkaUtils.createDirectStream[String, String](
        streamingContext,
        LocationStrategies.PreferConsistent,
        ConsumerStrategies.Assign[String, String](fromdbOffset.keys, kafkaParams, fromdbOffset)
      )
    }

    stream.foreachRDD(kafkaRdd => {
      val offsetRangs: Array[OffsetRange] = kafkaRdd.asInstanceOf[HasOffsetRanges].offsetRanges

      if(!kafkaRdd.isEmpty()) {

        val fileds: RDD[Array[String]] = kafkaRdd.map(_.value().split(" "))
        fileds.foreachPartition(it => {
          it.foreach(x => {
            println(x.toBuffer)
          })
        })
        CalculateUtils.calculateIncome(fileds)
        CalculateUtils.calculateItem(fileds)
        CalculateUtils.calculateIncomeZone(fileds, broadcastRules)
        DB.localTx(
          implicit session => {
            for (or <- offsetRangs) {
              SQL("replace into `offset` (groupId,topic,`partition`,untilOffset) values(?,?,?,?)")
                .bind(groupId,or.topic,or.partition,or.untilOffset).update().apply()
            }
          }
        )
      }
    })

    streamingContext.start()
    streamingContext.awaitTermination()

  }

-----------------------------------------------------------------------------------------------------------

解压redis源码包到/usr/local/src
tar -zxvf redis-3.2.11.tar.gz -C /usr/local/src/
安装c编译器
yum -y install gcc
进入源码包目录
make && make install
在/usr/local/下创建一个redis目录，然后拷贝源码包下的redis.conf到/usr/local/redis
修改redis.conf
daemonize yes #redis后台运行
appendonly #开启aof日志，它每次写操作都记录一条日志
bind 192.168.56.101
启动redis
redis-server /usr/local/redis/redis.conf

redis-cli -p 6379
config set requirepass 123

REDIS操作
keys *
set xiaoniu 1
set xiaoniu 5
get 'xiaoniu'
incr 'xiaoniu'  加1
incrby xiaoniu 100 加100
decr xiaoniu  减1
decrby xiaoniu 100  减100

-----------------------------------------------------------------------------------------------------------

redis集群

安装ruby

1、sudo yum install curl  安装curl
 2、http://www.rvm.io/ 官网首页就可以看到 
 gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB
 3、下载rvm   \curl -sSL https://get.rvm.io | bash -s stable 
 4、查找配置文件 find / -name rvm.sh 
 5、配置文件生效 source /etc/profile.d/rvm.sh 
 6、下载rvm依赖 rvm requirements 
 7、查看rvm库ruby版本 rvm list known
 8、安装ruby指定版本 rvm install ruby-2.4.1
 9、使用ruby版本默认 rvm use 2.4.1 default
--------------------- 
作者：Jabony 
来源：CSDN 
原文：https://blog.csdn.net/jabony/article/details/79977140 
版权声明：本文为博主原创文章，转载请附上博文链接！

gem install redis

在各个机器上
vi redis.conf
bind 一定要绑定ip地址，不要绑主机名
port 7000
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes

分别启动redis

在安装ruby的那台机器上/usr/local/src/redis/src
./redis-trib.rb create --replicas 1 192.168.56.101:7000 192.168.56.102:6379 192.168.56.103:6379 192.168.56.101:7001 192.168.56.104:6379 192.168.56.105:6379
前三个为主节点，后三个为从节点

停止redis集群
分别在节点上执行kill -9 进程号，一定是-9，强制停止

客户端连接集群
redis-cli -c -h 192.168.56.101 -p 7000
进入后输入cluter nodes查看集群节点

---------------------------------------

jedis连接池连接redis
package cn

import java.util

import redis.clients.jedis.{JedisCluster, _}

import scala.collection.mutable

/**
  * Created by zx on 2017/10/30.
  */


object JedisConnectPool {

  private val config: JedisPoolConfig = new JedisPoolConfig()

  //最大连接数
  config.setMaxTotal(20)
  //最大空闲连接数
  config.setMaxIdle(10)
  //当调用borrow Object方法时，是否进行有效性检查 -->
  config.setTestOnBorrow(true)

  //private val pool: JedisPool = new JedisPool(config, "192.168.100.101", 6379, 10000, "")

  val jedisClusterNodes = new util.HashSet[HostAndPort]()
  //Jedis Cluster will attempt to discover cluster nodes automatically
  jedisClusterNodes.add(new HostAndPort("192.168.100.101", 6379))
  jedisClusterNodes.add(new HostAndPort("192.168.100.102", 6379))
  jedisClusterNodes.add(new HostAndPort("192.168.100.103", 6379))
  jedisClusterNodes.add(new HostAndPort("192.168.100.104", 6379))
  jedisClusterNodes.add(new HostAndPort("192.168.100.105", 6379))
  jedisClusterNodes.add(new HostAndPort("192.168.100.106", 6379))
  val jedisCluster = new JedisCluster(jedisClusterNodes)

  def main(args: Array[String]): Unit = {
    val str = jedisCluster.get("abc123")
    println(str)
  }
}

------------------------------------------

redis扩容
先新增两个节点192.168.56.102:8001（预计当主） 192.168.56.102:8007（预计当从）
添加8001节点
./redis-trib.rb add-node 192.168.56.102:8001 192.168.56.102:6379
							新节点               集群中已存在的master节点
然后查看cluster nodes
记住新节点的id b54291991a9c6f77274462ff37a78cd6334db670
b539dc70eecad953474e9893c0413496091d8931 192.168.56.103:6379 myself,master - 0 0 8 connected 10923-16383
7d840c73f97040f64ff7379d9f662e80cfe3786b 192.168.56.101:7001 slave b539dc70eecad953474e9893c0413496091d8931 0 1552265445630 8 connected
b54291991a9c6f77274462ff37a78cd6334db670 192.168.56.102:8001 master - 0 1552265444126 0 connected
f0898fe0ae3b813e547f2548d552e430a97d37a5 192.168.56.101:7000 slave ddcb5c850180ef07555f46cc6e45c6d17aed6c2a 0 1552265444628 9 connected
30b41d8a01ece0e0771a1e488f8eeba73a341b19 192.168.56.102:6379 master - 0 1552265446133 14 connected 5461-10922
98ea450d82190aa8af16928c76baf4c5ccab6f9d 192.168.56.105:6379 slave 30b41d8a01ece0e0771a1e488f8eeba73a341b19 0 1552265445129 14 connected
ddcb5c850180ef07555f46cc6e45c6d17aed6c2a 192.168.56.104:6379 master - 0 1552265444126 9 connected 0-5460

然后给新主节点分配槽位
./redis-trib.rb reshard 192.168.56.102:8001
How many slots do you want to move (from 1 to 16384)? 4000  //分配4000个槽位
What is the receiving node ID? b54291991a9c6f77274462ff37a78cd6334db670  //输入分配的id
Please enter all the source node IDs.
  Type 'all' to use all the nodes as source nodes for the hash slots.
  Type 'done' once you entered all the source nodes IDs.
Source node #1:all  //从所有主节点分配抽取

添加8007节点./redis-trib.rb add-node 192.168.56.102:8007 192.168.56.102:6379
redis-cli -c -h 192.168.56.102 -p 8007 进入8007
输入CLUSTER REPLICATE b54291991a9c6f77274462ff37a78cd6334db670
						8001的节点id
						分配8007做8001的从节点


./redis-trib.rb check 192.168.56.101:6379 检查集群状态
--------------------------------------

redis收缩
修改/usr/local/src/redis/src/redis-trib.rb
找到move_slot
while true
            keys = source.r.cluster("getkeysinslot",slot,o[:pipeline])
            break if keys.length == 0
            begin
                source.r.client.call(["migrate",target.info[:host],target.info[:port],"",0,@timeout,"replace",:keys,*keys])  //要替换
				STDOUT.flush
            rescue => e
                if o[:fix] && e.to_s =~ /BUSYKEY/
                    xputs "*** Target key exists. Replacing it for FIX."
                    source.r.client.call(["migrate",target.info[:host],target.info[:port],"",0,@timeout,:replace,:keys,*keys])  //要替换
                else
                    puts ""
                    xputs "[ERR] Calling MIGRATE: #{e}"
                    exit 1
                end
            end

替换成
source.r.call(["migrate",target.info[:host],target.info[:port],"",0,@timeout,"replace",:keys,*keys])
  source.r.call(["migrate",target.info[:host],target.info[:port],"",0,@timeout,:replace,:keys,*keys])
-------------------------
这是由于ruby 和 redis gem版本不对应问题
https://blog.csdn.net/m0_37128231/article/details/80755478

删除8007从节点

98ea450d82190aa8af16928c76baf4c5ccab6f9d 192.168.56.105:6379 slave 30b41d8a01ece0e0771a1e488f8eeba73a341b19 0 1552268003333 14 connected
30b41d8a01ece0e0771a1e488f8eeba73a341b19 192.168.56.102:6379 master - 0 1552268003837 14 connected 6795-10922
7d840c73f97040f64ff7379d9f662e80cfe3786b 192.168.56.101:7001 myself,slave b539dc70eecad953474e9893c0413496091d8931 0 0 7 connected
ddcb5c850180ef07555f46cc6e45c6d17aed6c2a 192.168.56.104:6379 master - 0 1552268003837 9 connected 1333-5460
b539dc70eecad953474e9893c0413496091d8931 192.168.56.103:6379 master - 0 1552268002827 8 connected 12182-16383
79c72857431b4f9ca12ff5a1ade43500d8d4b8f7 192.168.56.102:8007 slave b54291991a9c6f77274462ff37a78cd6334db670 0 1552268003333 15 connected
f0898fe0ae3b813e547f2548d552e430a97d37a5 192.168.56.101:7000 slave ddcb5c850180ef07555f46cc6e45c6d17aed6c2a 0 1552268004341 9 connected
b54291991a9c6f77274462ff37a78cd6334db670 192.168.56.102:8001 master - 0 1552268002322 15 connected 0-1332 5461-6794 10923-12181

./redis-trib.rb del-node 192.168.56.102:6379 79c72857431b4f9ca12ff5a1ade43500d8d4b8f7
							对一个master节点 忘记8007节点。 8007节点id
							对6379节点做忘记下线节点的操作，那么经过一段时间，集群中的其他节点也都会忘记。
将8001主节点上的数据全部从新分配
 ./redis-trib.rb reshard 192.168.56.102:8001
 How many slots do you want to move (from 1 to 16384)? 3926 //要移动的个数12181 - 10923 + 1 = 1259  6794 - 5461 + 1 = 1334 1332 +1 = 1333
What is the receiving node ID? 30b41d8a01ece0e0771a1e488f8eeba73a341b19  //移动到102:6379上 master
Please enter all the source node IDs.
  Type 'all' to use all the nodes as source nodes for the hash slots.
  Type 'done' once you entered all the source nodes IDs.
Source node #1:45ad068248b2cac5d381759afe89de82dca25995  //数据源id是8001的id b54291991a9c6f77274462ff37a78cd6334db670
Source node #2:done  //写done

./redis-trib.rb del-node 192.168.56.102:6379 b54291991a9c6f77274462ff37a78cd6334db670
													8001节点id
redis客户端解决中文乱码问题  在redis-cli后加 --raw
-----------------------------------------------------------------------------------------------------------

Spark On Yarn
启动yarn
start-yarn.sh 
再另外一台机器上
yarn-daemon.sh start resourcemanager

设置HADOOP_CONF_DIR=/usr/local/hadoop-2.8.5/etc/hadoop环境变量

----------------------------

cluster模式运行
bin/spark-submit --class org.apache.spark.examples.SparkPi  --master yarn --deploy-mode cluster --driver-memory 1g --executor-memory 1g --executor-cores 2 --queue default examples/jars/spark-examples_2.11-2.4.0.jar 10000

yarn的ApplicationMaster会充当Driver的角色，如果挂掉，yarn会重启一个，sparkSubmit只是提交一下任务
看结果到yarn的管理界面application的logs里去看

----------------------------

client模式运行
bin/spark-shell --master yarn --deploy-mode client --driver-memory 1g --executor-memory 1g --executor-cores 2 --queue default 

spark-submit提交程序到yarn，yarn启动ApplicationMaster（其实就是ExecutorLauncher进程）申请资源，spark executor启动后直接连接spark-submit相当于Driver

-----------------
如果想让yarn运行client模式需要在所有yarn节点的yarn-site.xml文件中添加
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
解除内存检查

------------------------------------------------------------------------------------------------------------------

如何优雅的停止sparkstreaming on yarn
在Dirvier程序中写
var stopFlag:Boolean = false
val shutdownMarker = "hdfs://hdp12/tmp/shutdownmarker"

def main(args: Array[String]): Unit = {
	...
	streamingContext.start()

    val checkIntervalMillis = 10000
    var isStopped = false


    while (! isStopped) {
      println("calling awaitTerminationOrTimeout")
      isStopped = streamingContext.awaitTerminationOrTimeout(checkIntervalMillis)
      if (isStopped)
        println("confirmed! The streaming context is stopped. Exiting application...")
      else
        println("Streaming App is still running. Timeout...")
      checkShutdownMarker
      if (!isStopped && stopFlag) {
        println("stopping ssc right now")
        streamingContext.stop(true, true)
        println("ssc is stopped!!!!!!!")
      }
    }
}

def checkShutdownMarker = {
    if (!stopFlag) {
      val fs = FileSystem.get(new Configuration())
      stopFlag = fs.exists(new Path(shutdownMarker))
    }

 }

原理就是不断检查hdfs中的标记文件，如果文件存在，就停止

To shutdown the streaming app gracefully, place a file named "shutdownmarker" to HDFS /tmp folder
hdfs dfs -put shutdownmarker /tmp/shutdownmarker

https://github.com/lanjiang/streamingstopgraceful

----------------------------------------------------------------------------------------------------------------

mongodb是一个NoSQL数据库，里面存储的是json（bson），支持集群，高可用，可扩展
mongodb中的一些概念
	MongoDB            MySQL
	database          database
	collection        table
	json              二维表
	不支持SQL          SQL
	_id               主键
	支持建索引         支持建索引

---------------------------------------------------------------------------------------------------------------

安装mongodb单机版
Create a /etc/yum.repos.d/mongodb-org-4.0.repo file so that you can install MongoDB directly using yum:
[mongodb-org-4.0]
name=MongoDB Repository
baseurl=https://repo.mongodb.org/yum/redhat/$releasever/mongodb-org/4.0/x86_64/
gpgcheck=1
enabled=1
gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc
--------------------------
sudo yum install -y mongodb-org
--------------------------
mkdir -p /var/lib/mongo
mkdir -p /var/log/mongodb
--------------------------
systemctl start mongod
systemctl stop mongod
--------------------------
vi /etc/mongod.conf

bindIp: 192.168.56.103
security.authorization: enabled
--------------------------

use admin
db.createUser(
  {
    user: "myUserAdmin",
    pwd: "abc123",
    roles: [ { role: "userAdminAnyDatabase", db: "admin" }, "readWriteAnyDatabase" ]
  }
)

---------------------------

mongo --host 192.168.56.103 -u "admin" -p "az63091919" --authenticationDatabase "admin"

-------------------------------------------------------------------------------------------------------------

创建数据库
use mydb
mongodb创建表
db.createCollection("users")
删除表
db.bike.drop()

查询索引
db.inventory.getIndexes()
删除索引
db.inventory.dropIndex({"item":-1})

-------------------------------------------------------------------------------------------------------------

mongodb集群安装
卸载用rpm安装的mongodb
rpm -qa | grep mongo
rpm -e --nodeps 要卸载的软件包

rm -rf /var/lib/mongo
rm -rf /var/log/mongodb

rm -rf /etc/yum.repos.d/mongodb-org-4.0.repo 
yum clean all

关闭sellinux
vi /etc/selinux/config
将SELINUX=enforcing改为SELINUX=disabled 
reboot重启
查看sellinux状态 getenforce 应该显示Disabled

yum install libcurl openssl
tar -zxvf mongodb-linux-*-4.0.6.tgz
mv mongodb-linux-x86_64-rhel70-4.0.6/ mongodb
vi /etc/profile
添加
MONGODB_HOME=/usr/local/mongodb
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$MONGODB_HOME/bin
export JAVA_HOME HADOOP_HOME HADOOP_CONF_DIR MONGODB_HOME PATH

source /etc/profile

创建config服务器
mkdir -p /usr/local/mongo/config
vi mongod.conf 

# mongod.conf

# for documentation of all options, see:
#   http://docs.mongodb.org/manual/reference/configuration-options/

# where to write logging data.
systemLog:
  destination: file
  logAppend: true
  path: /usr/local/mongo/config/log/mongod.log

# Where and how to store data.
storage:
  dbPath: /usr/local/mongo/config/data
  journal:
    enabled: true
#  engine:
#  mmapv1:
#  wiredTiger:

# how the process runs
processManagement:
  fork: true  # fork and run in background
  pidFilePath: /usr/local/mongo/config/run/mongod.pid  # location of pidfile
  timeZoneInfo: /usr/share/zoneinfo

# network interfaces
net:
  port: 27019
  bindIp: 192.168.56.102  # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.


#security:

#operationProfiling:

replication:
  replSetName: configs

sharding:
  clusterRole: configsvr

## Enterprise-Only Options

#auditLog:

#snmp:

然后执行mongod -f /usr/local/mongo/config/mongod.conf启动config服务器
进入mongo客户端  mongo --host 192.168.56.101 --port 27019
输入
config = {
    _id : "configs",
     members : [
         {_id : 0, host : "192.168.56.101:27019" },
         {_id : 1, host : "192.168.56.102:27019" },
         {_id : 2, host : "192.168.56.103:27019" }
     ]
 }
初始化
rs.initiate(config)
查看
rs.status


创建分片
mkdir -p /mongo/shard1/{log,data,run}
cat >> /mongo/shard1/mongod.conf << EOF
systemLog:
  destination: file
  logAppend: true
  path: /mongo/shard1/log/mongod.log
storage:
  dbPath: /mongo/shard1/data
  journal:
    enabled: true
processManagement:
  fork: true
  pidFilePath: /mongo/shard1/run/mongod.pid
net:
  port: 27018
replication:
  replSetName: shard1
sharding:
  clusterRole: shardsvr
EOF

启动所有shard1 server
登陆任意一台shard1服务器(希望哪一台机器是主，就登录到那一台机器上)，初始化副本集
use admin
config = {
   _id : "shard1",
    members : [
        {_id : 0, host : "192.168.56.101:27018" },
        {_id : 1, host : "192.168.56.102:27018" },
        {_id : 2, host : "192.168.56.103:27018" }
    ]
}
rs.initiate(config)

按照同样方法创建shard2、shard3

创建mongos
mkdir /usr/local/mongo/mongos
vi mongod.conf
# mongod.conf

# for documentation of all options, see:
#   http://docs.mongodb.org/manual/reference/configuration-options/

# where to write logging data.
systemLog:
  destination: file
  logAppend: true
  path: /usr/local/mongo/mongos/log/mongod.log

# Where and how to store data.
#storage:
#  dbPath: /usr/local/mongo/shard1/data
#  journal:
#    enabled: true
#  engine:
#  mmapv1:
#  wiredTiger:

# how the process runs
processManagement:
  fork: true  # fork and run in background
  pidFilePath: /usr/local/mongo/mongos/run/mongod.pid  # location of pidfile
  timeZoneInfo: /usr/share/zoneinfo

# network interfaces
net:
  port: 27017
  bindIp: 192.168.56.102  # Enter 0.0.0.0,:: to bind to all IPv4 and IPv6 addresses or, alternatively, use the net.bindIpAll setting.


#security:

#operationProfiling:

#replication:

sharding:
  configDB: configs/192.168.56.101:27019,192.168.56.102:27019,192.168.56.103:27019
## Enterprise-Only Options

#auditLog:

#snmp:

mkdir -p /usr/local/mongo/mongos/{log,run}
用mongos命令启动mongos
mongos -f /usr/local/mongo/mongos/mongod.conf

登陆任意一台mongos
sh.addShard("shard1/192.168.56.101:27018,192.168.56.102:27018,192.168.56.103:27018")
sh.addShard("shard2/192.168.56.101:27020,192.168.56.102:27020,192.168.56.103:27020")
sh.addShard("shard3/192.168.56.101:27021,192.168.56.102:27021,192.168.56.103:27021")
rs.slaveOk()


然后可以使用了
use mobike
use admin
把mobike数据库加入分片
db.runCommand({"enablesharding":"mobike"})
use mobike
db.createCollection("bikes")
use admin
按数据集bikes按照_id进行hash分片
db.runCommand({"shardcollection":"mobike.bikes","key":{_id:"hashed"}})

进入shard1的从节点要执行rs.slaveOk(),从节点才可以读,默认从节点不能读

mongodb集群设置用户名密码
openssl rand -base64 756 > /usr/local/mongodb/keyFile.file
chmod 400 /usr/local/mongodb/keyFile.file
第一条命令是生成密钥文件，第二条命令是使用chmod更改文件权限，为文件所有者提供读权限
将keyFile传到另外两台机器上

随便进入一台mongos
use admin
db.createUser(
    {
        user:"admin",
        pwd:"az63091919",
        roles:[{role:"root",db:"admin"}]
    }
)
进入shard的主机
admin.createUser(
  {
    user: "shard1",
    pwd: "az63091919",
    roles: [ { role: "userAdminAnyDatabase", db: "admin" } ]
  }
)
shard2和shard3也这样执行

依次对每台机器执行
killall mongod
killall mongos

修改configserver和shard的配置文件
添加
security:
  keyFile: /usr/local/mongodb/keyFile.file
  authorization: enabled
修改mongos server的配置文件
添加
security:
  keyFile: /usr/local/mongodb/keyFile.file

重启集群
mongod -f /usr/local/mongodb/conf/config.conf
mongod -f /usr/local/mongodb/conf/shard1.conf
mongod -f /usr/local/mongodb/conf/shard2.conf
mongod -f /usr/local/mongodb/conf/shard3.conf
mongos -f /usr/local/mongodb/conf/mongos.conf

进入mongos
use admin
db.auth("admin","az63091919")
就可以使用数据库了

进入shard1
db.getSiblingDB("admin").auth("shard1", "az63091919" )
创建可以读分片数据的用户
db.getSiblingDB("admin").createUser(
  {
    "user" : "shard1user",
    "pwd" : "az63091919",
    roles: [ { "role" : "readWriteAnyDatabase", "db" : "admin" } ]
  }
)
读分片数据就用shard1user这个用户auth
db.getSiblingDB("admin").auth("shard1user", "az63091919" )

　　readAnyDatabase：任何数据库的只读权限(和read相似)

　　readWriteAnyDatabase：任何数据库的读写权限(和readWrite相似)

　　userAdminAnyDatabase：任何数据库用户的管理权限(和userAdmin相似)

　　dbAdminAnyDatabase：任何数据库的管理权限(dbAdmin相似)

　　__system：  什么权限都有

db.grantRolesToUser( "admin" , [ { role: "clusterAdmin", db: "admin" }])
-------------------------------------------------------------------------------------------------------------

pv: PageView所有页面的访问次数（哪些页面访问的次数多，按从高到低进行排序）
uv: UniqueView 独立访问用户

将这些用户访问log记录到mongo中，然后用spark分析

在页面中埋点（将用户重要的行为记录下来）
将用户的行为数据存储到后台

-------------------------------------------------------------------------------------------------------------

安装nginx单机版
sbin/nginx  启动
sbin/nginx -s stop   停止

1.上传nginx安装包
2.解压nginx
	tar -zxvf nginx-1.12.2.tar.gz -C /usr/local/src/
3.进入到nginx的源码目录
	cd /usr/local/src/nginx-1.12.2/
4.预编译
	./configure
5.安静gcc编译器
	yum -y install gcc pcre-devel openssl openssl-devel
6.然后再执行
	./configure
7.编译安装nginx
	make && make install
8.启动nginx
	sbin/nginx
9.查看nginx进程
	ps -ef | grep nginx
	netstat -anpt | grep nginx

--------------------------------

springboot程序打包，如果用的是jsp技术的话，要改成打war包
<groupId>com.wyd</groupId>
	<artifactId>bike</artifactId>
	<version>0.0.1-SNAPSHOT</version>
	<packaging>war</packaging>
	<name>bike</name>
java -jar niubike-0.0.1-SNAPSHOT.war >> ./logs 2>&1 &

-------------------------------

安装带kafka插件的nginx单机
1.安装git
	yum install -y git
2.切换到/usr/local/src目录，然后将kafka的c客户端源码clone到本地
	cd /usr/local/src
	git clone https://github.com/edenhill/librdkafka
3.进入到librdkafka，然后进行编译
	cd librdkafka
	yum install -y gcc gcc-c++ pcre-devel zlib-devel
	./configure
	make && make install

4.安装nginx整合kafka的插件，进入到/usr/local/src，clone nginx整合kafka的源码
	cd /usr/local/src
	git clone https://github.com/brg-liuwei/ngx_kafka_module

5.进入到nginx的源码包目录下	（编译nginx，然后将将插件同时编译）
	cd /usr/local/src/nginx-1.12.2
	./configure --add-module=/usr/local/src/ngx_kafka_module/
	make
	make install
6.启动nginx，报错，找不到kafka.so.1的文件
	error while loading shared libraries: librdkafka.so.1: cannot open shared object file: No such file or directory
7.加载so库
	echo "/usr/local/lib" >> /etc/ld.so.conf
	ldconfig

8. vi nginx/conf/nignx.conf
添加
	#gzip on
	kafka;
    kafka_broker_list node2:9092 node3:9092 node4:9092;
    server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location /kafka/track {
           kafka_topic track;
        }
        location /kafka/user {
           kafka_topic user;
        }
    }

---------------------------------------------------------------------------------------------------------------------

flume和logstash都是日志采集工具，flume是用java写的  logstash是用Jruby写的

---------------------------------------------------------------------------------------------------------------------

flume source的生命周期
先执行构造器，然后执行configure方法，然后执行start方法，processor.process
1.读取配置文件 （读取哪个文件，编码集、偏移量写到哪个文件，多长时间检测一下文件是否有新内容）

---------------------------------------------------------------------------------------------------------------------

安装flume，解压即可
创建dir-hdfs.conf文件

写spooldir类型source
#定义三大组件
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#配置source组件，使用spooldir source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = ~/log/spooldir
a1.sources.r1.channels = c1
#控制每行读取的字节大小，默认2048
a1.sources.r1.deserializer.maxLineLength = 10000

#配置sink组件，使用hdfs sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M%S
a1.sinks.k1.hdfs.filePrefix = events-
#配置文件切换规则
a1.sinks.k1.hdfs.rollInterval = 120
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 10000
#配置存储目录切换
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
#配置批量写入hdfs
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.useLocalTimeStamp = true

#配置channel组件，用内存组件
a1.channels.c1.type = memory
a1.channels.c1.capacity = 10000
a1.channels.c1.transactionCapacity = 5000
a1.channels.c1.byteCapacityBufferPercentage = 20
a1.channels.c1.byteCapacity = 800000

-----------------------------------------

flume如果要连接hdfs集群，就要把core-sit和hdfs-site复制到flume/conf文件夹下

-----------------------------------------

启动flume
bin/flume-ng agent --conf conf --conf-file dir-hdfs.conf --name a1 -Dflume.root.logger=INFO,console 
后台启动flume
nohup bin/flume-ng agent --conf conf --conf-file dir-hdfs.conf --name a1 1>dev/null 2>&1 &

-----------------------------------------

写exec类型sources
#定义三大组件
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#定义sources，使用exec
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /root/log/secure/access.log
a1.sources.r1.channels = c1

#配置sink组件，使用hdfs sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M%S
a1.sinks.k1.hdfs.filePrefix = events-
#配置文件切换规则
a1.sinks.k1.hdfs.rollInterval = 60
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 10000
#配置存储目录切换
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
#配置批量写入hdfs
a1.sinks.k1.hdfs.batchSize = 20
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.useLocalTimeStamp = true

#配置channel组件，用内存组件
a1.channels.c1.type = memory
a1.channels.c1.capacity = 100
a1.channels.c1.transactionCapacity = 50

测试
while true do echo `date` >> access.log sleep 0.5 done

----------------------------------------------------------------------------------------------------------------------

avro：一种通用的跨平台跨语言的序列号协议
protobuf：google序列号协议
jdk的serializable
hadoop的Writable

--------------------------------------------------------------------------------------------------------------------

多级agent串联，定义node1上的agent
#定义三大组件
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#定义sources，使用exec
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /root/log/secure/access.log
a1.sources.r1.channels = c1

#使用avro类型的sink，相当于avro客户端
a1.sinks.k1.type = avro
a1.sinks.k1.channel = c1
a1.sinks.k1.hostname = node2
a1.sinks.k1.port = 4141

#配置channel组件，用内存组件
a1.channels.c1.type = memory
a1.channels.c1.capacity = 600
a1.channels.c1.transactionCapacity = 300

-----------------------

定义node2上的agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

#定义source，使用avro类型source，相当于服务端
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
a1.sources.r1.bind = node2
a1.sources.r1.port = 4141

配置sink组件，使用hdfs sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M%S
a1.sinks.k1.hdfs.filePrefix = events-
#配置文件切换规则
a1.sinks.k1.hdfs.rollInterval = 60
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 10000
#配置存储目录切换
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
#配置批量写入hdfs
a1.sinks.k1.hdfs.batchSize = 20
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.useLocalTimeStamp = true

#配置channel组件，用内存组件
a1.channels.c1.type = memory
a1.channels.c1.capacity = 600
a1.channels.c1.transactionCapacity = 300

-----------------------------------------------------------------------------------------------------------------

自定义source
查看flumeclloect工程下的com.wyd.flume.source.TailFileSource

将工程打包，然后把jar包放到flume/lib目录下
bin/flume-ng agent --conf conf --conf-file tail-roll-file-posit.conf --name a1 -Dflume.root.logger=INFO,console

a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = com.wyd.flume.source.TailFileSource
a1.sources.r1.channels = c1
a1.sources.r1.filePath = /root/log/selfsource/access.log
a1.sources.r1.posiFile = /root/log/position/posit.log

a1.sinks.k1.type = file_roll
a1.sinks.k1.channel = c1
a1.sinks.k1.sink.directory = /root/log/result

a1.channels.c1.type = memory
a1.channels.c1.capacity = 100
a1.channels.c1.transactionCapacity = 50

-----------------------------------------------------------------------------------------------------------------

kafka channel使用，往kafka里写

a1.sources = r1
a1.channels = c1

#定义sources，使用exec
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /root/log/secure/access.log
a1.sources.r1.channels = c1

a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = node2:9092,node3:9092,node4:9092
a1.channels.c1.kafka.topic = c1
a1.channels.c1.kafka.consumer.group.id = flume-consumer
#写入kafka里的数据是文本而不是flume的event
a1.channels.c1.parseAsFlumeEvent = false

-------------------------------------------------------------------------------------------------------------------
自定义interceptor，
实现org.apache.flume.interceptor.Interceptor接口
参照SearchAndReplaceInterceptor写
将程序打包上传到flume的lib目录下
interceptor只能跟在source后面

a1.sources = r1
a1.channels = c1

#定义sources，使用exec
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /root/log/interceptortest/access.log
a1.sources.r1.channels = c1
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.wyd.flume.interceptor.JsonInterceptor$JsonBuilder
a1.sources.r1.interceptors.i1.fields = id,name,age,fv
a1.sources.r1.interceptors.i1.separator = ,

a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = node2:9092,node3:9092,node4:9092
a1.channels.c1.kafka.topic = c1
#写入kafka里的数据是文本而不是flume的event
a1.channels.c1.parseAsFlumeEvent = false

----------------------------------------------------------------------------------------------------------------------

使用taildirsource
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = TAILDIR
a1.sources.r1.channels = c1
a1.sources.r1.positionFile = ./taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /root/log/tailfile/.*.log
a1.sources.r1.fileHeader = false
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.wyd.flume.interceptor.JsonInterceptor$JsonBuilder
a1.sources.r1.interceptors.i1.fields = id,name,age,fv
a1.sources.r1.interceptors.i1.separator = ,

a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = node2:9092,node3:9092,node4:9092
a1.channels.c1.kafka.topic = tail1
a1.channels.c1.parseAsFlumeEvent = false

配置sink组件，使用hdfs sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M%S
a1.sinks.k1.hdfs.filePrefix = events-
#配置文件切换规则
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 10
#配置存储目录切换
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
#配置批量写入hdfs
a1.sinks.k1.hdfs.batchSize = 10
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.useLocalTimeStamp = true

能实现多级目录监测的taildir

https://github.com/qwurey/flume-source-taildir-recursive

---------------------------------------------------------------------------------------------------------------------

从kafka读数据存到hdfs中

a1.sinks = k1
a1.channels = c1

a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = node2:9092,node3:9092,node4:9092
a1.channels.c1.kafka.topic = recharge
a1.channels.channel1.kafka.consumer.group.id = recharge-consumer
a1.channels.c1.parseAsFlumeEvent = false

配置sink组件，使用hdfs sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/mobike/recharge/%y-%m-%d/%H%M%S
a1.sinks.k1.hdfs.filePrefix = events-
#配置文件切换规则
a1.sinks.k1.hdfs.rollInterval = 0
a1.sinks.k1.hdfs.rollSize = 262144000
a1.sinks.k1.hdfs.rollCount = 0
#配置存储目录切换
a1.sinks.k1.hdfs.round = true
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
#配置批量写入hdfs
a1.sinks.k1.hdfs.batchSize = 10
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.useLocalTimeStamp = true

---------------------------------------------------------------------------------------------------------------------
spring data的mongodb注解
@Document(collection = "users")  //这个类映射到Mongo中的users集合
public class User

@Id //主键
private String id
@Indexed(unique = true)  //这个字段创建索引，并且唯一
private String phoneNum
@Transient  //这个字段在数据库中不存储
private String verifyCode
//地理位置字段，里面保存着经纬度 geohash索引
@GeoSpatialIndexed(type = GeoSpatialIndexType.GEO_2DSPHERE)

----------------------------------------------------------------------------------------------------------------

mongo geohash 2dsphere要求数据经度在前，纬度在后
location: [longitude,latitude]
否则报错
Can't extract geo keys

----------------------------------------------------------------------------------------------------------------

Structured Streaming详解
https://blog.csdn.net/l_15156024189/article/details/81612860
Spark Structured Streaming + Kafka
https://blog.csdn.net/asd136912/article/details/82913264

---------------------------------------------------------------------------------------------------------------

在spark streaming中使用sparksql
package com.sid.spark
 
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.streaming.{Seconds, StreamingContext, Time}
 
/**
  * Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
  * network every second.
  *
  * Usage: SqlNetworkWordCount <hostname> <port>
  * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
  *
  * To run this on your local machine, you need to first run a Netcat server
  *    `$ nc -lk 9999`
  * and then run the example
  *    `$ bin/run-example org.apache.spark.examples.streaming.SqlNetworkWordCount localhost 9999`
  *
  *    Spark Streaming整合Spark SQL完成词频统计
  */
 
object SqlNetworkWordCount {
  def main(args: Array[String]) {
 
    val sparkConf = new SparkConf().setMaster("local[2]").setAppName("SocketWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(5))
 
    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream("node1", 6789)
    val words = lines.flatMap(_.split(" "))
 
    // Convert RDDs of the words DStream to DataFrame and run SQL query
    words.foreachRDD { (rdd: RDD[String], time: Time) =>
      // Get the singleton instance of SparkSession
      val spark = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
      import spark.implicits._
 
      // Convert RDD[String] to RDD[case class] to DataFrame
      val wordsDataFrame = rdd.map(w => Record(w)).toDF()
 
      // Creates a temporary view using the DataFrame
      wordsDataFrame.createOrReplaceTempView("words")
 
      // Do word count on table using SQL and print it
      val wordCountsDataFrame =
        spark.sql("select word, count(*) as total from words group by word")
      println(s"========= $time =========")
      wordCountsDataFrame.show()
    }
 
    ssc.start()
    ssc.awaitTermination()
  }
 
  /** Case class for converting RDD to DataFrame */
  case class Record(word: String)
 
 
  /** Lazily instantiated singleton instance of SparkSession */
  object SparkSessionSingleton {
 
    @transient  private var instance: SparkSession = _
    def getInstance(sparkConf: SparkConf): SparkSession = {
      if (instance == null) {
        instance = SparkSession
          .builder
          .config(sparkConf)
          .getOrCreate()
      }
      instance
    }
  }
}

------------------------------------------------------------------------------------------------------------------

分布式内存：内存对齐

吴恩达机器学习 https://study.163.com/course/introduction/1004570029.htm

-----------------------------------------------------------------------------------

RDD内部包含的5个主要的属性
1、包含分区列表
2、包含一个针对每个split的计算函数
3、对其他RDD的依赖列表
4、可选，如果是KeyValueRDD的话，可以带分区类
5、可选，首选块位置列表

---------------------------------------------------------------------------

spark解决数据倾斜

设置随机分区





















